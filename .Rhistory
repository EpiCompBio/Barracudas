else {
if (is.null(folds) || !is.numeric(folds) || folds <
2 || folds > n)
stop("Invalid number of folds.")
else {
M = round(folds)
folds = split(sample(1:n), rep(1:M, length = n))
}
}
} else {
folds = split(1:n, rep(1:n, length = n))
M = n
}
RSS = rbind(rep(n - 1, q), matrix(nrow = ncomp, ncol = q))
RSS.indiv = array(NA, c(n, q, ncomp + 1))
PRESS.inside = Q2.inside = matrix(nrow = ncomp, ncol = q)
press.mat = Ypred = array(NA, c(n, q, ncomp))
MSEP = R2 = matrix(NA, nrow = q, ncol = ncomp)
rownames(MSEP) = rownames(R2) = colnames(Q2.inside) = colnames(Y)
dimnames(press.mat)[[2]] = colnames(Y)
stop.user = FALSE
if (progressBar == TRUE)
pb <- txtProgressBar(style = 3)
if (progressBar == TRUE)
setTxtProgressBar(pb, i/M)
omit = folds[[i]]
if (length(omit) == 1)
stop.user = TRUE
X.train = X[-omit, ]
Y.train = Y[-omit, ]
X.test = matrix(X[omit, ], nrow = length(omit))
Y.test = matrix(Y[omit, ], nrow = length(omit))
spls.res = sPLS(X.train, Y.train, ncomp, mode, max.iter,
tol, keepX = keepX, keepY = keepY)
for (k in 1:ncomp) {
featuresX[[k]] = c(unlist(featuresX[[k]]), selectVar(spls.res,
comp = k)$X$name)
featuresY[[k]] = c(unlist(featuresY[[k]]), selectVar(spls.res,
comp = k)$Y$name)
}
Y.hat = predict(spls.res, X.test)$predict
Y.hat
for (h in 1:ncomp) {
Ypred[omit, , h] = Y.hat[, , h]
press.mat[omit, , h] = (Y.test - Y.hat[, , h])^2
RSS.indiv[omit, , h + 1] = (Y.test - Y.hat[,
, h])^2
}
press.mat
set.seed(setseed)
X = object$X
Y = object$Y
tol = object$tol
max.iter = object$max.iter
keepX = object$keepX
keepY = object$keepY
mode = object$mode
ncomp = object$ncomp
n = nrow(X)
p = ncol(X)
q = ncol(Y)
res = list()
validation = match.arg(validation)
featuresX = featuresY = list()
for (k in 1:ncomp) {
featuresX[[k]] = featuresY[[k]] = NA
}
if (length(dim(X)) != 2)
stop("'X' must be a numeric matrix for validation.")
if (object$mode == "canonical")
stop("sPLS mode should be set to regression, invariant or classic")
if (any(criterion == "Q2") & ncomp == 1)
stop("'ncomp' must be > 1 for Q2 criterion.")
if (any(is.na(X)) || any(is.na(Y)))
stop("Missing data in 'X' and/or 'Y'. Use 'nipals' for dealing with NAs.")
if (validation == "Mfold") {
if (is.list(folds)) {
if (length(folds) < 2 | length(folds) > n)
stop("Invalid number of folds.")
if (length(unique(unlist(folds))) != n)
stop("Invalid folds.")
M = length(folds)
}
else {
if (is.null(folds) || !is.numeric(folds) || folds <
2 || folds > n)
stop("Invalid number of folds.")
else {
M = round(folds)
folds = split(sample(1:n), rep(1:M, length = n))
}
}
} else {
folds = split(1:n, rep(1:n, length = n))
M = n
}
RSS = rbind(rep(n - 1, q), matrix(nrow = ncomp, ncol = q))
RSS.indiv = array(NA, c(n, q, ncomp + 1))
PRESS.inside = Q2.inside = matrix(nrow = ncomp, ncol = q)
press.mat = Ypred = array(NA, c(n, q, ncomp))
MSEP = R2 = matrix(NA, nrow = q, ncol = ncomp)
rownames(MSEP) = rownames(R2) = colnames(Q2.inside) = colnames(Y)
dimnames(press.mat)[[2]] = colnames(Y)
stop.user = FALSE
if (progressBar == TRUE)
pb <- txtProgressBar(style = 3)
for (i in 1:M) {
if (progressBar == TRUE)
setTxtProgressBar(pb, i/M)
omit = folds[[i]]
if (length(omit) == 1)
stop.user = TRUE
X.train = X[-omit, ]
Y.train = Y[-omit, ]
X.test = matrix(X[omit, ], nrow = length(omit))
Y.test = matrix(Y[omit, ], nrow = length(omit))
spls.res = sPLS(X.train, Y.train, ncomp, mode, max.iter,
tol, keepX = keepX, keepY = keepY)
# for (k in 1:ncomp) {
#   featuresX[[k]] = c(unlist(featuresX[[k]]), selectVar(spls.res,
#                                                        comp = k)$X$name)
#   featuresY[[k]] = c(unlist(featuresY[[k]]), selectVar(spls.res,
#                                                        comp = k)$Y$name)
# }
Y.hat = predict(spls.res, X.test)$predict
for (h in 1:ncomp) {
Ypred[omit, , h] = Y.hat[, , h]
press.mat[omit, , h] = (Y.test - Y.hat[, , h])^2
RSS.indiv[omit, , h + 1] = (Y.test - Y.hat[,
, h])^2
}
}
press.mat
for (h in 1:ncomp) {
MSEP[, h] = apply(as.matrix(press.mat[, , h]), 2,
mean, na.rm = TRUE)
R2[, h] = (diag(cor(Y, Ypred[, , h], use = "pairwise")))^2
if (q > 1) {
RSS[h + 1, ] = t(apply(RSS.indiv[, , h + 1],
2, sum))
PRESS.inside[h, ] = colSums(press.mat[, , h],
na.rm = TRUE)
}
else {
RSS[h + 1, q] = sum(RSS.indiv[, q, h + 1])
PRESS.inside[h, q] = sum(press.mat[, q, h], na.rm = TRUE)
}
Q2.inside[h, ] = 1 - PRESS.inside[h, ]/RSS[h, ]
}
PRESS.inside
perf
perf.sPLS()
perf.sPLS
predict.sPLS()
predict.sPLS
if(is.null(chosen_seed_first)) {
set.seed(1)
} else {
set.seed(chosen_seed_first)
}
MSEP_df_nrepeat=as.data.frame(matrix(0,nrow=ncol(sPLS_obj$Y),ncol=sPLS_obj$ncomp))
if (is.null(nCores)) {
nCores=detectCores()
}
set.seed(chosen_seed_first)
MSEP_df=as.data.frame(matrix(0,nrow=ncol(sPLS_obj$Y),ncol=sPLS_obj$ncomp))
PRESS_df=as.data.frame(matrix(0,nrow=ncol(sPLS_obj$Y),ncol=sPLS_obj$ncomp))
RSS_df=as.data.frame(matrix(0,nrow=ncol(sPLS_obj$Y),ncol=sPLS_obj$ncomp+1))
RSS_df[,1]=rep(nrow(sPLS_obj$X)-1,ncol(sPLS_obj$Y))
Q2_df=as.data.frame(matrix(0,nrow=ncol(sPLS_obj$Y),ncol=sPLS_obj$ncomp))
rows_folds=createFolds(1:nrow(sPLS_obj$X), k = folds, list = TRUE, returnTrain = FALSE)
# rows_folds=segments
for (k in 1:length(rows_folds)) {
X_train=sPLS_obj$X[-rows_folds[[k]],]
X_test=sPLS_obj$X[rows_folds[[k]],]
Y_train=sPLS_obj$Y[-rows_folds[[k]],]
Y_test=sPLS_obj$Y[rows_folds[[k]],]
sPLS_train=sgPLS::sPLS(X=X_train,Y=Y_train,keepX=sPLS_obj$keepX,keepY=sPLS_obj$keepY,mode="regression",ncomp = 2,scale=TRUE)
# sPLS_train=mixOmics::spls(X=X_train,Y=Y_train,keepX=sPLS_obj$keepX,keepY=sPLS_obj$keepY,mode="regression",ncomp = 2)
sPLS_test_pred=predict.sPLS(sPLS_train,newdata = X_test)
# X_test%*%sPLS_train$mat.c%*%t(sPLS_train$loadings$Y)
deflated_matrix=sPLS_obj$Y
for (comp in 1:sPLS_obj$ncomp) {
MSEP_df[,comp]=MSEP_df[,comp] +  length(rows_folds[[k]])*colMeans((Y_test - sPLS_test_pred$predict[,,comp])^2)
PRESS_df[,comp]=PRESS_df[,comp] + colSums((Y_test - sPLS_test_pred$predict[,,comp])^2)
if (k==1) {
regression_coeffs_deflation=t(deflated_matrix)%*%TmpsPLS$variates$X[,comp,drop=FALSE]/
(t(TmpsPLS$variates$X[,comp,drop=FALSE])%*%(TmpsPLS$variates$X[,comp,drop=FALSE]))[1,1]
deflated_matrix=deflated_matrix - TmpsPLS$variates$X[,comp,drop=FALSE]%*%t(regression_coeffs_deflation)
RSS_df[,comp+1]=RSS_df[,comp+1]=colSums(deflated_matrix^2)
}
}
}
1 - PRESS_df / RSS_df[,1:sPLS_obj$ncomp]
PRESS_df
TmpsPLS <- sgPLS::sPLS(X,Y,keepX=c(12,12), keepY=c(4,4),ncomp=2,mode='regression')
test_perf=perf.sPLS(TmpsPLS,validation='Mfold',folds=10,nrepeat=1,progressBar=FALSE)
TmpsPLS <- sgPLS::sPLS(X,Y,keepX=c(12,12), keepY=c(4,4),ncomp=2,mode='regression')
test_perf=perf.sPLS(TmpsPLS,validation='Mfold',folds=10,nrepeat=1,progressBar=FALSE)
perf.sPLS()
perf.sPLS
selectVar
getAnywhere(selectVar)
test_perf=perf(TmpsPLS,validation='Mfold',folds=10,nrepeat=1,progressBar=FALSE)
test_perf=sgPLS::perf.sPLS(TmpsPLS,validation='Mfold',folds=10,nrepeat=1,progressBar=FALSE)
TmpsPLS <- sgPLS::sPLS(X,Y,keepX=c(12,12), keepY=c(4,4),ncomp=2,mode='regression')
test_perf=sgPLS::perf.sPLS(TmpsPLS,validation='Mfold',folds=10,nrepeat=1,progressBar=FALSE)
sPLS_pref_self=function (object, criterion = c("all", "MSEP", "R2", "Q2"), validation = c("Mfold",
"loo"), folds = 10, progressBar = TRUE, setseed = 1, ...)
{
set.seed(setseed)
X = object$X
Y = object$Y
tol = object$tol
max.iter = object$max.iter
keepX = object$keepX
keepY = object$keepY
mode = object$mode
ncomp = object$ncomp
n = nrow(X)
p = ncol(X)
q = ncol(Y)
res = list()
validation = match.arg(validation)
featuresX = featuresY = list()
for (k in 1:ncomp) {
featuresX[[k]] = featuresY[[k]] = NA
}
if (length(dim(X)) != 2)
stop("'X' must be a numeric matrix for validation.")
if (object$mode == "canonical")
stop("sPLS mode should be set to regression, invariant or classic")
if (any(criterion == "Q2") & ncomp == 1)
stop("'ncomp' must be > 1 for Q2 criterion.")
if (any(is.na(X)) || any(is.na(Y)))
stop("Missing data in 'X' and/or 'Y'. Use 'nipals' for dealing with NAs.")
if (validation == "Mfold") {
if (is.list(folds)) {
if (length(folds) < 2 | length(folds) > n)
stop("Invalid number of folds.")
if (length(unique(unlist(folds))) != n)
stop("Invalid folds.")
M = length(folds)
}
else {
if (is.null(folds) || !is.numeric(folds) || folds <
2 || folds > n)
stop("Invalid number of folds.")
else {
M = round(folds)
folds = split(sample(1:n), rep(1:M, length = n))
}
}
} else {
folds = split(1:n, rep(1:n, length = n))
M = n
}
RSS = rbind(rep(n - 1, q), matrix(nrow = ncomp, ncol = q))
RSS.indiv = array(NA, c(n, q, ncomp + 1))
PRESS.inside = Q2.inside = matrix(nrow = ncomp, ncol = q)
if (any(criterion %in% c("all", "MSEP", "R2", "Q2"))) {
press.mat = Ypred = array(NA, c(n, q, ncomp))
MSEP = R2 = matrix(NA, nrow = q, ncol = ncomp)
rownames(MSEP) = rownames(R2) = colnames(Q2.inside) = colnames(Y)
dimnames(press.mat)[[2]] = colnames(Y)
stop.user = FALSE
if (progressBar == TRUE)
pb <- txtProgressBar(style = 3)
for (i in 1:M) {
if (progressBar == TRUE)
setTxtProgressBar(pb, i/M)
omit = folds[[i]]
if (length(omit) == 1)
stop.user = TRUE
X.train = X[-omit, ]
Y.train = Y[-omit, ]
X.test = matrix(X[omit, ], nrow = length(omit))
Y.test = matrix(Y[omit, ], nrow = length(omit))
spls.res = sPLS(X.train, Y.train, ncomp, mode, max.iter,
tol, keepX = keepX, keepY = keepY)
# for (k in 1:ncomp) {
#   featuresX[[k]] = c(unlist(featuresX[[k]]), selectVar(spls.res,
#                                                        comp = k)$X$name)
#   featuresY[[k]] = c(unlist(featuresY[[k]]), selectVar(spls.res,
#                                                        comp = k)$Y$name)
# }
Y.hat = predict(spls.res, X.test)$predict
for (h in 1:ncomp) {
Ypred[omit, , h] = Y.hat[, , h]
press.mat[omit, , h] = (Y.test - Y.hat[, , h])^2
RSS.indiv[omit, , h + 1] = (Y.test - Y.hat[,
, h])^2
}
}
if (stop.user == TRUE & validation == "Mfold")
stop("The folds value was set too high to perform cross validation. Choose validation = \"loo\" or set folds to a lower value")
for (h in 1:ncomp) {
MSEP[, h] = apply(as.matrix(press.mat[, , h]), 2,
mean, na.rm = TRUE)
R2[, h] = (diag(cor(Y, Ypred[, , h], use = "pairwise")))^2
if (q > 1) {
RSS[h + 1, ] = t(apply(RSS.indiv[, , h + 1],
2, sum))
PRESS.inside[h, ] = colSums(press.mat[, , h],
na.rm = TRUE)
}
else {
RSS[h + 1, q] = sum(RSS.indiv[, q, h + 1])
PRESS.inside[h, q] = sum(press.mat[, q, h], na.rm = TRUE)
}
Q2.inside[h, ] = 1 - PRESS.inside[h, ]/RSS[h, ]
}
colnames(MSEP) = colnames(R2) = rownames(Q2.inside) = paste("ncomp",
c(1:ncomp), sep = " ")
rownames(MSEP) = rownames(R2) = colnames(Q2.inside) = colnames(Y)
if (q == 1)
rownames(MSEP) = rownames(R2) = ""
if (ncomp > 1) {
if (q > 1) {
Q2.total = 1 - rowSums(PRESS.inside, na.rm = TRUE)/rowSums(RSS[-(ncomp +
1), ], na.rm = TRUE)
}
else {
Q2.total = t(1 - PRESS.inside/RSS[-(ncomp + 1),
])
}
}
else {
Q2.total = NA
}
names(Q2.total) = paste("comp", 1:ncomp, sep = " ")
}
print(PRESS.inside)
# if (progressBar == TRUE)
#   cat("\n")
# list.featuresX = list.featuresY = list()
# for (k in 1:ncomp) {
#   remove.naX = which(is.na(featuresX[[k]]))
#   remove.naY = which(is.na(featuresY[[k]]))
#   list.featuresX[[k]] = sort(summary(as.factor(featuresX[[k]][-remove.naX]))/M,
#                              decreasing = TRUE)
#   list.featuresY[[k]] = sort(summary(as.factor(featuresY[[k]][-remove.naY]))/M,
#                              decreasing = TRUE)
# }
# features.finalX = features.finalY = list()
# for (k in 1:ncomp) {
#   features.finalX[[k]] = selectVar(object, comp = k)$X$value
#   features.finalY[[k]] = selectVar(object, comp = k)$Y$value
# }
# names(features.finalX) = names(features.finalY) = names(list.featuresX) = names(list.featuresY) = paste("comp",
#                                                                                                         1:ncomp)
# if (any(criterion %in% c("all", "MSEP")))
#   res$MSEP = MSEP
# if (any(criterion %in% c("all", "R2")))
#   res$R2 = R2
# if (any(criterion %in% c("all", "Q2")))
#   res$Q2 = t(Q2.inside)
# if (any(criterion %in% c("all", "Q2")))
#   res$Q2.total = Q2.total
# res$features$stable.X = list.featuresX
# res$features$stable.Y = list.featuresY
# res$features$final.X = features.finalX
# res$features$final.Y = features.finalY
# res$press.mat = press.mat
# res$RSS.indiv = RSS.indiv
# res$PRESS.inside = PRESS.inside
# res$RSS = RSS
# method = "pls.mthd"
# class(res) = c("perf", method)
# return(invisible(res))
}
test_perf=sPLS_pref_self(TmpsPLS,validation='Mfold',folds=10,nrepeat=1,progressBar=FALSE)
TmpsPLS <- mixOmics::spls(X,Y,keepX=c(12,12), keepY=c(4,4),ncomp=2,mode='regression')
TmpPerf <- perf(TmpsPLS,validation='Mfold',folds=10,nrepeat=1,progressBar=FALSE)
print(TmpPerf$PRESS)
mixOmics::perf
getAnywhere(mixOmics::perf)
perf
sPLS_obj=TmpsPLS
folds=5
nrepeat=10
chosen_seed_first=500
set.seed(chosen_seed_first)
MSEP_df=as.data.frame(matrix(0,nrow=ncol(sPLS_obj$Y),ncol=sPLS_obj$ncomp))
PRESS_df=as.data.frame(matrix(0,nrow=ncol(sPLS_obj$Y),ncol=sPLS_obj$ncomp))
RSS_df=as.data.frame(matrix(0,nrow=ncol(sPLS_obj$Y),ncol=sPLS_obj$ncomp+1))
RSS_df[,1]=rep(nrow(sPLS_obj$X)-1,ncol(sPLS_obj$Y))
Q2_df=as.data.frame(matrix(0,nrow=ncol(sPLS_obj$Y),ncol=sPLS_obj$ncomp))
rows_folds=createFolds(1:nrow(sPLS_obj$X), k = folds, list = TRUE, returnTrain = FALSE)
for (k in 1:length(rows_folds)) {
X_train=sPLS_obj$X[-rows_folds[[k]],]
X_test=sPLS_obj$X[rows_folds[[k]],]
Y_train=sPLS_obj$Y[-rows_folds[[k]],]
Y_test=sPLS_obj$Y[rows_folds[[k]],]
sPLS_train=sgPLS::sPLS(X=X_train,Y=Y_train,keepX=sPLS_obj$keepX,keepY=sPLS_obj$keepY,mode="regression",ncomp = 2,scale=TRUE)
# sPLS_train=mixOmics::spls(X=X_train,Y=Y_train,keepX=sPLS_obj$keepX,keepY=sPLS_obj$keepY,mode="regression",ncomp = 2)
sPLS_test_pred=predict.sPLS(sPLS_train,newdata = X_test)
# X_test%*%sPLS_train$mat.c%*%t(sPLS_train$loadings$Y)
deflated_matrix=sPLS_obj$Y
for (comp in 1:sPLS_obj$ncomp) {
MSEP_df[,comp]=MSEP_df[,comp] +  length(rows_folds[[k]])*colMeans((Y_test - sPLS_test_pred$predict[,,comp])^2)
PRESS_df[,comp]=PRESS_df[,comp] + colSums((Y_test - sPLS_test_pred$predict[,,comp])^2)
if (k==1) {
regression_coeffs_deflation=t(deflated_matrix)%*%TmpsPLS$variates$X[,comp,drop=FALSE]/
(t(TmpsPLS$variates$X[,comp,drop=FALSE])%*%(TmpsPLS$variates$X[,comp,drop=FALSE]))[1,1]
deflated_matrix=deflated_matrix - TmpsPLS$variates$X[,comp,drop=FALSE]%*%t(regression_coeffs_deflation)
RSS_df[,comp+1]=RSS_df[,comp+1]=colSums(deflated_matrix^2)
}
}
}
PRESS_df
X = as.matrix(predictors)
n = nrow(X)
p = ncol(X)
Y = as.matrix(responses)
q = ncol(Y)
nc = comps
#Scale X and Y (always for PLS, can rescale at the end if necessary)
X.old = scale(X)
Y.old = scale(Y)
#Initialize all the matices that are used in the PLS algorithm
Wh = matrix(0, p, nc)
Uh = matrix(0, n, nc)
Th = matrix(0, n, nc)
Ch = matrix(0, q, nc)
Ph = matrix(0, p, nc)
bh = rep(0, nc)
RSS = rbind(rep(n - 1, q), matrix(NA, nc, q))
PRESS = matrix(NA, nc, q)
Q2 = matrix(NA, nc, q)
sets_size = c(rep(n%/%10, 9), n - 9 * (n%/%10))
obs = sample(1:n, size = n)
segments = vector("list", length = 10)
ini = cumsum(sets_size) - sets_size + 1
fin = cumsum(sets_size)
for (k in 1:10) {
segments[[k]] = obs[ini[k]:fin[k]]
}
#Loop over the number of components desired. There cannot be more components than predictor variables
for (h in 1:nc) {
#Initialize the first column of u (Y scores)
#You would do that generally with the response variable that has the highest variance, but any works
u.new = Y.old[, 1]
w.old = rep(1, p)
iter = 1
#Initialize the distance measure between two weight esimates (big enough so the condition always fails)
w.dif=1000
while (sum(w.dif^2) > 1e-06 && iter < 100) {
#Step 1 : Build weights
w.new = t(X.old) %*% u.new/sum(u.new^2)
#Step 2 : Standardize weights
w.new = w.new/sqrt(sum(w.new^2))
#Step 3 : Build the scores
t.new = X.old %*% w.new
#Step 4 : Build the weights related to Y matrice
c.new = t(Y.old) %*% t.new/sum(t.new^2)
# Some algorithm do normalize here, some do not
# c.new =  c.new / sqrt(sum(c.new^2))
#Step 5 : Build the scores related to the Y matrice
u.new = Y.old %*% c.new/sum(c.new^2)
#Step 6 : Get the difference between the newly calculated weights and the old ones to assess convergence
w.dif = w.new - w.old
w.old = w.new
iter = iter + 1 #Update iterations
}
# Build loadings after convergence
p.new = t(X.old) %*% t.new/sum(t.new^2)
# Some algorithm prefer to normalize the loadings over other things
# p.old = p.new
# p.new = p.new / sqrt(sum(p.old^2))
# t.new = t.new * sqrt(sum(p.old^2))
# w.new = w.new * sqrt(sum(p.old^2))
#CROSS VALIDATION - Calculate PRESS AND Q^2
RSS[h + 1,] = colSums((Y.old - t.new %*% t(c.new))^2)
press = matrix(0, 10, q)
for (i in 1:10) {
aux = segments[[i]]
uh.si = Y.old[-aux, 1]
wh.siold = rep(1, p)
itcv = 1
repeat {
wh.si = t(X.old[-aux, ]) %*% uh.si/sum(uh.si^2)
wh.si = wh.si/sqrt(sum(wh.si^2))
th.si = X.old[-aux, ] %*% wh.si
ch.si = t(Y.old[-aux, ]) %*% th.si/sum(th.si^2)
uh.si = Y.old[-aux, ] %*% ch.si/sum(ch.si^2)
wsi.dif = wh.si - wh.siold
wh.siold = wh.si
if (sum(wsi.dif^2) < 1e-6 || itcv == 100)
break
itcv = itcv + 1
}
Yhat.si = (X.old[aux, ] %*% wh.si) %*% t(ch.si)
press[i, ] = colSums((Y.old[aux, ] - Yhat.si)^2)
}
PRESS[h,] = colSums(press)
Q2[h,] = 1 - (PRESS[h, ]/RSS[h, ])
# Deflate both matrices
bh[h] = t(u.new) %*% t.new / sum(t.new^2)
X.old = X.old - (t.new %*% t(p.new))
Y.old = Y.old - (bh[h] * t.new %*% t(c.new))
print(t.new)
print(head(Y.old))
Wh[, h] = w.new
Uh[, h] = u.new
Th[, h] = t.new
Ch[, h] = c.new
Ph[, h] = p.new
#Regression coefficient between the two built scores; this is useful to build regression coefficients
}
PRESS
save.image("~/Imperial College 2018-2019/save_space.RData")
